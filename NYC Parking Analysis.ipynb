{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"NYC Parking\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-30-0-31-67.ec2.internal:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>NYC Parking</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f7811ff19b0>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Summons Number: string, Plate ID: string, Registration State: string, Issue Date: string, Violation Code: string, Vehicle Body Type: string, Vehicle Make: string, Violation Precinct: string, Issuer Precinct: string, Violation Time: string]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NYC = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/common_folder/nyc_parking/Parking_Violations_Issued_-_Fiscal_Year_2017.csv\")\n",
    "NYC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|Summons Number|Plate ID|Registration State|Issue Date|Violation Code|Vehicle Body Type|Vehicle Make|Violation Precinct|Issuer Precinct|Violation Time|\n",
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|    5092469481| GZH7067|                NY|2016-07-10|             7|             SUBN|       TOYOT|                 0|              0|         0143A|\n",
      "|    5092451658| GZH7067|                NY|2016-07-08|             7|             SUBN|       TOYOT|                 0|              0|         0400P|\n",
      "|    4006265037| FZX9232|                NY|2016-08-23|             5|             SUBN|        FORD|                 0|              0|         0233P|\n",
      "|    8478629828| 66623ME|                NY|2017-06-14|            47|             REFG|       MITSU|                14|             14|         1120A|\n",
      "|    7868300310| 37033JV|                NY|2016-11-21|            69|             DELV|       INTER|                13|             13|         0555P|\n",
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NYC.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+--------+------------------+----------+------------------+------------------+------------------+------------------+-----------------+-----------------+\n",
      "|summary|     Summons Number|Plate ID|Registration State|Issue Date|    Violation Code| Vehicle Body Type|      Vehicle Make|Violation Precinct|  Issuer Precinct|   Violation Time|\n",
      "+-------+-------------------+--------+------------------+----------+------------------+------------------+------------------+------------------+-----------------+-----------------+\n",
      "|  count|           10803028|10803028|          10803028|  10803028|          10803028|          10803028|          10803028|          10803028|         10803028|         10803028|\n",
      "|   mean| 6.81744702906579E9|Infinity|              99.0|      null|34.599430455979565|3.9258887134586864| 6519.974025974026| 45.01216260848347|46.82931211508477|909.2857142857143|\n",
      "| stddev|2.320233962328227E9|     NaN|               0.0|      null|19.359868716323483|0.5013415469252528|18091.257389147086|  40.5525602684357|62.66703577269572|791.8453853409226|\n",
      "|    min|         1002884949|   #1MOM|                99|1972-03-30|                 0|                00|             ,FREI|                 0|                0|            .240P|\n",
      "|    max|         8585600044|       ~|                WY|2069-11-19|                99|               nan|               nan|                99|              997|              nan|\n",
      "+-------+-------------------+--------+------------------+----------+------------------+------------------+------------------+------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# summary statistics\n",
    "NYC.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Summons Number: string (nullable = true)\n",
      " |-- Plate ID: string (nullable = true)\n",
      " |-- Registration State: string (nullable = true)\n",
      " |-- Issue Date: string (nullable = true)\n",
      " |-- Violation Code: string (nullable = true)\n",
      " |-- Vehicle Body Type: string (nullable = true)\n",
      " |-- Vehicle Make: string (nullable = true)\n",
      " |-- Violation Precinct: string (nullable = true)\n",
      " |-- Issuer Precinct: string (nullable = true)\n",
      " |-- Violation Time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# datatype of columns\n",
    "NYC.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10803028"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of Rows\n",
    "NYC.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of Columns\n",
    "len(NYC.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10803028"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop duplicate values\n",
    "NYC = NYC.dropDuplicates()\n",
    "NYC.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10803028"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If Any, drop null values\n",
    "NYC = NYC.dropna()\n",
    "NYC.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10803028"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total Number of summons\n",
    "NYC.select('Summons Number').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|Summons_Number|Plate_ID|Registration_State|Issue_Date|Violation_Code|Vehicle_Body_Type|Vehicle_Make|Violation_Precinct|Issuer_Precinct|Violation_Time|\n",
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "|    5093739601|   9L43B|                NY|2016-10-03|             7|             TAXI|       NISSA|                 0|              0|         0100A|\n",
      "|    4632064841| GZJ3137|                NY|2017-03-24|            36|             4DSD|       DODGE|                 0|              0|         0100P|\n",
      "|    4007108080|  S78CKS|                NJ|2017-03-29|             5|             4 DR|       NISSA|                 0|              0|         0100P|\n",
      "|    4632094092|  V43GPX|                NJ|2017-03-24|            36|             4 DR|        FORD|                 0|              0|         0100P|\n",
      "|    4629164960|  Y10NKQ|                FL|2017-01-04|            36|               4D|       NISSA|                 0|              0|         0100P|\n",
      "+--------------+--------+------------------+----------+--------------+-----------------+------------+------------------+---------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# replace \" \" with \"_\" in column names\n",
    "\n",
    "NYC = NYC.toDF(*(c.replace(' ','_') for c in NYC.columns))\n",
    "NYC.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary table view\n",
    "NYC.createOrReplaceTempView(\"NYC_parkingTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Summons_Number: string, Plate_ID: string, Registration_State: string, Issue_Date: string, Violation_Code: string, Vehicle_Body_Type: string, Vehicle_Make: string, Violation_Precinct: string, Issuer_Precinct: string, Violation_Time: string]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('Select * from NYC_parkingTable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('Select * from NYC_parkingTable').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Number of tickets for each year\n",
    "year_wise_data =  spark.sql('Select Year(Issue_date) as year, count(Summons_Number) as Number_Of_Tickets from NYC_parkingTable group by year order by year desc')\n",
    "year_wise_data.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary : \n",
    "    - Data is from 1972 to 2069.\n",
    "    - It is concentrated around 2016 & 2017.\n",
    "    - We will consider data for 2017 only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_wise_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need data for 2017 Only. \n",
    "Filter the data for 2017 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering data for 2017\n",
    "NYC = spark.sql(\"select * from NYC_parkingTable where Year(Issue_Date) = 2017 \")\n",
    "NYC.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYC.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating new view for 2017 data only\n",
    "NYC.createOrReplaceTempView(\"NYC_2017\")\n",
    "\n",
    "#Showing distribution \n",
    "Distribution_on_years= spark.sql(\"\"\"\n",
    "                                 SELECT year(Issue_Date) as year,month(Issue_Date) as month,count(*) as Ticket_Frequency\n",
    "                                 FROM NYC_2017 GROUP BY year(Issue_Date),month(Issue_Date) order by Ticket_Frequency desc\n",
    "                                 \"\"\")\n",
    "Distribution_on_years.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum number of violations are in the month of May. It has been observed that from July to December, there is a significant drop in number of violations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Number_of_Violations_by_month = Distribution_on_years.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.clf()\n",
    "Number_of_Violations_by_month.plot(x= 'month', y='Ticket_Frequency', kind='bar')\n",
    "plt.title(\"Violations on the basis of month in 2017\")\n",
    "plt.xlabel('month')\n",
    "plt.ylabel('Ticket_Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Checking_null_values=spark.sql(\"\"\"Select count(*) as Number_of_Null_Values from NYC_2017 \n",
    "                                  where Summons_Number is NULL or Plate_ID is NULL \n",
    "                                  or Registration_State is NULL or Issue_Date is NULL \n",
    "                                  or Violation_Code is NULL \n",
    "                                  or Vehicle_Body_Type is NULL \n",
    "                                  or Vehicle_Make is NULL \n",
    "                                  or Violation_Precinct is NULL \n",
    "                                  or Issuer_Precinct is NUll \n",
    "                                  or Violation_Time is NULL \"\"\")\n",
    "Checking_null_values.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There is no field with null value.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking on Plate_ID field to know if there are cases with same plate id.\n",
    "\n",
    "Plate_Id_Check=spark.sql(\"\"\"Select Plate_ID, count(*) as Ticket_Frequency \n",
    "                            from NYC_2017 group by Plate_ID having count(*)>1 order by Ticket_Frequency desc\"\"\")\n",
    "Plate_Id_Check.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There is one value'BLANKPLATE' which we cannot track. Therefore, we can remove this.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NYC=NYC[NYC.Plate_ID!='BLANKPLATE']\n",
    "NYC.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the view\n",
    "NYC.createOrReplaceTempView(\"NYC_2017\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see number of violations above 500\n",
    "Plate_Id_Above_500=spark.sql(\"\"\"Select Plate_ID, count(*) as Ticket_Frequency from NYC_2017 \n",
    "                                group by Plate_ID having count(*)>=500 order by Ticket_Frequency desc\"\"\")\n",
    "Plate_Id_Above_500.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot number of violation above 500\n",
    "Number_of_Violations_By_PlateID=Plate_Id_Above_500.toPandas()\n",
    "plt.clf()\n",
    "Number_of_Violations_By_PlateID.plot(x= 'Plate_ID', y='Ticket_Frequency', kind='bar')\n",
    "plt.title(\"Number of Violations above 500 \")\n",
    "plt.xlabel('Plate_ID')\n",
    "plt.ylabel('Ticket_Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are 7 Plate ID with more than 500 violations.**\n",
    "\n",
    "###  Questions to Be Answered in the Analysis\n",
    "\n",
    "The following analysis should be performed on PySpark mounted on your CoreStack cluster, using the PySpark library. Remember that you need to summarise the analysis with your insights along with the code\n",
    "\n",
    "### Examine the data\n",
    "\n",
    "#### Q1. Find the total number of tickets for the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select count(*) as Number_Of_Tickets_for_Year from NYC_2017').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2. Find out the number of unique states from where the cars that got parking tickets came.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2 = spark.sql(\"\"\"\n",
    "          SELECT Registration_state, count(*) as Number_of_records \n",
    "          FROM NYC_2017\n",
    "          group by Registration_state\n",
    "          order by Number_of_records\n",
    "          \"\"\")\n",
    "q2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2.show(65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is a state which contains \"99\" as value. We will replace this with the state which has maximum number of tickets.\n",
    "- NY is the state with Max number of tickets.\n",
    "- Replacing \"99\" with \"NY\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when,lit\n",
    "NYC=NYC.withColumn('Registration_State',when(NYC[\"Registration_State\"]==\"99\",lit('NY')).otherwise(NYC[\"Registration_State\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update the NYC_2017 Table\n",
    "NYC.createOrReplaceTempView(\"NYC_2017\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2 = spark.sql(\"\"\"\n",
    "          SELECT Registration_state, count(*) as Number_of_records \n",
    "          FROM NYC_2017\n",
    "          group by Registration_state\n",
    "          order by Number_of_records\n",
    "          \"\"\")\n",
    "q2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unique registration states are now 64. \"99\" has been replaced with \"NY\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation tasks\n",
    "\n",
    "#### Q1. How often does each violation code occur? Display the frequency of the top five violation codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Violation code count\n",
    "from pyspark.sql.functions import count,desc,countDistinct\n",
    "NYC.select(countDistinct(\"Violation_Code\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Frequency of occuring each violation code\n",
    "spark.sql(\"\"\"\n",
    "          SELECT Violation_code, count(*) as Violation_code_count\n",
    "          from NYC_2017\n",
    "          group by Violation_code\n",
    "          order by Violation_code_count desc\n",
    "          \"\"\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Frequency top 5 violations\n",
    "spark.sql(\"\"\"\n",
    "          SELECT Violation_code, count(*) as Violation_code_count\n",
    "          from NYC_2017\n",
    "          group by Violation_code\n",
    "          order by Violation_code_count desc\n",
    "          \"\"\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2. How often does each 'vehicle body type' get a parking ticket? How about the 'vehicle make'?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each vehicle body type get a parking ticket\n",
    "vehicleBodyType = spark.sql(\"SELECT Vehicle_Body_Type, count(*) as Ticket_Frequency from NYC_2017 group by Vehicle_Body_Type order by Ticket_Frequency desc\")\n",
    "vehicleBodyType.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vehicle Make\n",
    "\n",
    "vehicleMake = spark.sql(\"SELECT Vehicle_Make, count(*) as Ticket_Frequency from NYC_2017 group by Vehicle_Make order by Ticket_Frequency desc\")\n",
    "vehicleMake.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3. A precinct is a police station that has a certain zone of the city under its command. Find the (5 highest) frequencies of tickets for each of the following:**\n",
    "\n",
    "**1.'Violation Precinct' (This is the precinct of the zone where the violation occurred). Using this, can you draw any insights for parking violations in any specific areas of the city?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT Violation_Precinct, count(*) as Ticket_Frequency \n",
    "            from NYC_2017 group by Violation_Precinct order by Ticket_Frequency desc\"\"\").show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you would have noticed that the dataframe has the'Violating Precinct' as '0'. These are erroneous entries. Hence, you need to provide the records for five correct precincts.\n",
    "\n",
    "**2 'Issuer Precinct' (this is the precinct that issued the ticket)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT Issuer_Precinct, count(*) as Ticket_Frequency \n",
    "            from NYC_2017 group by Issuer_Precinct order by Ticket_Frequency desc\"\"\").show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here, you would have noticed that the dataframe has the 'Issuing Precinct' as '0'. These are erroneous entries. Hence, you need to provide the records for five correct precincts. (Hint: Print the top six entries after sorting.)**\n",
    "\n",
    "- So the top 5 area where most violation occurs are 19, 14, 1, 18 and 114.\n",
    "- Similarily, the top 5 Issuer Precient are 19, 14, 1, 18 and 114"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4. Find the violation code frequencies for three precincts that have issued the most number of tickets. Do these precinct zones have an exceptionally high frequency of certain violation codes? Are these codes common across precincts? **\n",
    "\n",
    "**4.1 Finding violation code frequency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "violation_code_freq = spark.sql(\"\"\"select Issuer_Precinct,Violation_Code, count(*) as Frequency \n",
    "                                    from NYC_2017 group by Issuer_Precinct, Violation_Code order by Frequency desc\"\"\" )\n",
    "violation_code_freq.show(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not considering 0. Therefore 18,19,14 are the three issuer precincts with maximum number of violations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets dive into the Issuer Precinct one by one\n",
    "# Issuer Precinct 18 here\n",
    "violation_code_freq_18 = spark.sql(\"\"\"select Violation_Code, count(*) as Frequency \n",
    "                                    from NYC_2017 where Issuer_Precinct=18 group by Violation_Code order by Frequency desc\"\"\" )\n",
    "violation_code_freq_18.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Issuer Precinct 19 here\n",
    "violation_code_freq_19 = spark.sql(\"select Violation_Code, count(*) as Frequency from NYC_2017 where Issuer_Precinct=19 group by Violation_Code order by Frequency desc\" )\n",
    "violation_code_freq_19.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Issuer Precinct 14 here\n",
    "violation_code_freq_14 = spark.sql(\"select Violation_Code, count(*) as Frequency from NYC_2017 where Issuer_Precinct=14 group by Violation_Code order by Frequency desc\" )\n",
    "violation_code_freq_14.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2 Common codes across precincts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_codes =spark.sql(\"select Violation_Code, count(*) as Frequency from NYC_2017 where Issuer_Precinct in (18,19,14) group by Violation_Code order by Frequency desc\")\n",
    "common_codes.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "- Precinct 18 and Precinct 14 has more less similar top violation code.\n",
    "- But Precinct 19 has very different top violation code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5.Find out the properties of parking violations across different times of the day:\n",
    "- Find a way to deal with missing values, if any. (Hint: Check for the null values using 'isNull' under the SQL. Also, to remove the null values, check the 'dropna' command in the API documentation.)\n",
    "\n",
    "- The Violation Time field is specified in a strange format. Find a way to make this a time attribute that you can use to divide into groups.\n",
    "\n",
    "- Divide 24 hours into six equal discrete bins of time. Choose the intervals as you see fit. For each of these groups, find the three most commonly occurring violations. (Hint: Use the CASE-WHEN in SQL view to segregate into bins. To find the most commonly occurring violations, you can use an approach similar to the one mentioned in the hint for question 4.)\n",
    "\n",
    "- Now, try another direction. For the three most commonly occurring violation codes, find the most common time of the day (in terms of the bins from the previous part)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of missing values\n",
    "spark.sql(\"SELECT count(*) as No_of_Count_Values from NYC_2017 WHERE Violation_Time is NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for the null value\n",
    "from pyspark.sql.functions import col\n",
    "NYC.where(col(\"Violation_Time\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Divide 24 hours into six equal discrete bins of time.\n",
    "bins=spark.sql(\"\"\"SELECT Summons_Number, Violation_Code , Violation_Time, Issuer_Precinct, \n",
    "                  case when substring(Violation_Time,1,2) in ('00','01','02','03','12') \n",
    "                  and upper(substring(Violation_Time,-1))='A' then 1 \n",
    "                  when substring(Violation_Time,1,2) in ('04','05','06','07') \n",
    "                  and upper(substring(Violation_Time,-1))='A' then 2 \n",
    "                  when substring(Violation_Time,1,2) in ('08','09','10','11') \n",
    "                  and upper(substring(Violation_Time,-1))='A' then 3 \n",
    "                  when substring(Violation_Time,1,2) in ('12','00','01','02','03') \n",
    "                  and upper(substring(Violation_Time,-1))='P' then 4 \n",
    "                  when substring(Violation_Time,1,2) in ('04','05','06','07') \n",
    "                  and upper(substring(Violation_Time,-1))='P' then 5 \n",
    "                  when substring(Violation_Time,1,2) in ('08','09','10','11') \n",
    "                  and upper(substring(Violation_Time,-1))='P' then 6 \n",
    "                  else null end as Violation_Time_bin from NYC_2017 \n",
    "                  where Violation_Time is not null \n",
    "                  or (length(Violation_Time)=5 and upper(substring(Violation_Time,-1)) in ('A','P') \n",
    "                  and substring(Violation_Time,1,2) in ('00','01','02','03','04','05','06','07', '08','09','10','11','12'))\"\"\")\n",
    "bins.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bins Details\n",
    "\n",
    "Bin       Time Interval\n",
    "- 1         12:00 AM to 4:00 AM\n",
    "- 2         4:00 AM to 8:00 AM\n",
    "- 3         8:00 AM to 12:00 PM\n",
    "- 4         12:00 PM to 4:00 PM\n",
    "- 5         4:00 PM to 8:00 PM\n",
    "- 6         8:00 PM to 12:00 AM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins.createOrReplaceTempView(\"NYC_bins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# violation code time count\n",
    "violation_code_time_count = spark.sql(\"SELECT Violation_Code,Violation_Time_bin, count(*) count from NYC_bins group by Violation_Code,Violation_Time_bin\")\n",
    "violation_code_time_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin1 = spark.sql(\"select Violation_Code,count(*) Vio_cnt from NYC_bins where Violation_Time_bin == 1 group by Violation_Code order by Vio_cnt desc\")\n",
    "bin1.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin2 = spark.sql(\"select Violation_Code,count(*) Vio_cnt from NYC_bins where Violation_Time_bin == 2 group by Violation_Code order by Vio_cnt desc\")\n",
    "bin2.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin3 = spark.sql(\"select Violation_Code,count(*) Vio_cnt from NYC_bins where Violation_Time_bin == 3 group by Violation_Code order by Vio_cnt desc\")\n",
    "bin3.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin4 = spark.sql(\"select Violation_Code,count(*) Vio_cnt from NYC_bins where Violation_Time_bin == 4 group by Violation_Code order by Vio_cnt desc\")\n",
    "bin4.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin5 = spark.sql(\"select Violation_Code,count(*) Vio_cnt from NYC_bins where Violation_Time_bin == 5 group by Violation_Code order by Vio_cnt desc\")\n",
    "bin5.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin6 = spark.sql(\"select Violation_Code,count(*) Vio_cnt from NYC_bins where Violation_Time_bin == 6 group by Violation_Code order by Vio_cnt desc\")\n",
    "bin6.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try another direction. For the three most commonly occurring violation codes, find the most common time of the day (in terms of the bins from the previous part)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_bin = spark.sql(\"select Violation_Time_bin, count(*) Vio_count from NYC_bins where Violation_Code in (21, 36, 38) group by Violation_Time_bin order by Vio_count desc\")\n",
    "time_bin.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bins 3, 4, 5 are having most violations\n",
    "\n",
    "The obvious reason could be, In day time significantly more vehicles were running and hence more violations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6.Let’s try and find some seasonality in this data:**\n",
    "\n",
    "**a)First, divide the year into some number of seasons,and find frequencies of tickets for each season.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets_seasonality = spark.sql(\"\"\"\n",
    "                            select Violation_Code , Issuer_Precinct, \n",
    "                            case when MONTH(TO_DATE(Issue_Date, 'MM/dd/yyyy')) between 03 and 05 then 'spring' \n",
    "                            when MONTH(TO_DATE(Issue_Date, 'MM/dd/yyyy')) between 06 and 08 \n",
    "                            then 'summer' \n",
    "                            when MONTH(TO_DATE(Issue_Date, 'MM/dd/yyyy')) between 09 and 11 \n",
    "                            then 'autumn' \n",
    "                            when MONTH(TO_DATE(Issue_Date, 'MM/dd/yyyy')) in (1,2,12) then 'winter' \n",
    "                            else 'unknown' end  as season from NYC_2017\"\"\")\n",
    "tickets_seasonality.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Season Binning Details**\n",
    "- Season :    Month interval\n",
    "\n",
    "- spring    :March, April, May\n",
    "- summer    :June, July, August\n",
    "- autumn    :September, October, November\n",
    "- winter    :December, January, February"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets_seasonality.createOrReplaceTempView(\"NYC_tickets_seasonality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets_seasonality_freq = spark.sql(\"select season, count(*) as no_of_tickets from NYC_tickets_seasonality group by 1 order by 2 desc\")\n",
    "tickets_seasonality_freq.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spring season\n",
    "violation_spring = spark.sql(\"select Violation_Code, count(*) as Frequency from NYC_tickets_seasonality where Issuer_Precinct in (19, 14, 1) and season = 'spring' group by Violation_Code order by Frequency desc\" )\n",
    "violation_spring.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Winter season\n",
    "violation_winter = spark.sql(\"select Violation_Code, count(*) as Frequency from NYC_tickets_seasonality where Issuer_Precinct in (19, 14, 1) and season = 'winter' group by Violation_Code order by Frequency desc\" )\n",
    "violation_winter.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summer season\n",
    "violation_summer = spark.sql(\"select Violation_Code, count(*) as Frequency from NYC_tickets_seasonality where Issuer_Precinct in (19, 14, 1) and season = 'summer' group by Violation_Code order by Frequency desc\" )\n",
    "violation_summer.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autumn season\n",
    "violation_autumn = spark.sql(\"select Violation_Code, count(*) as Frequency from NYC_tickets_seasonality where Issuer_Precinct in (19, 14, 1) and season = 'autumn' group by Violation_Code order by Frequency desc\" )\n",
    "violation_autumn.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7. The fines collected from all the instances of parking violation constitute a source of revenue for the NYC Police Department. Let’s take an example of estimating this for the three most commonly occurring codes:**\n",
    "\n",
    "**a). Find total occurrences of the three most common violation codes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_Violation = spark.sql(\"select Violation_Code, count(*) as Frequency from NYC_2017 group by Violation_Code order by Frequency desc\")\n",
    "common_Violation.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b). Using this information, find the total amount collected for the three violation codes with maximum tickets. State the code which has the highest total collection.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "common_Violation_fine=common_Violation.withColumn(\"fine\",when(common_Violation.Violation_Code == 21, (common_Violation.Frequency) *55).otherwise((common_Violation.Frequency)*50))\n",
    "common_Violation_fine.show(3)\n",
    "print('Total collection = ',767740*55+662765*50+541526*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code with 21 had the highest collection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c).What can you intuitively infer from these findings?\n",
    "- Jan to June had the major violation & July to Dec has a drastic drop.\n",
    "- Highest violation &collection was by Code-21(No parking where parking is not allowed by sign, street marking or traffic control device.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
